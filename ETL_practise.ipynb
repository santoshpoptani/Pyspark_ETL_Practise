{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b196b98b-9c19-47b6-9794-4fcef486a863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in c:\\users\\santosh\\appdata\\roaming\\python\\python311\\site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\santosh\\appdata\\roaming\\python\\python311\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf24157f-7df0-44c5-8b63-a38364aeb27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.appName(\"ETL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfeaf942-a457-4aee-bc7a-b012fd9bf889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|dept_no|         dept_name|\n",
      "+-------+------------------+\n",
      "|   d009|  Customer Service|\n",
      "|   d005|       Development|\n",
      "|   d002|           Finance|\n",
      "|   d003|   Human Resources|\n",
      "|   d001|         Marketing|\n",
      "|   d004|        Production|\n",
      "|   d006|Quality Management|\n",
      "|   d008|          Research|\n",
      "|   d007|             Sales|\n",
      "+-------+------------------+\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "+------+-------+----------+----------+\n",
      "|emp_no|dept_no| from_date|   to_date|\n",
      "+------+-------+----------+----------+\n",
      "| 10001|   d005|1986-06-26|9999-01-01|\n",
      "| 10002|   d007|1996-08-03|9999-01-01|\n",
      "| 10003|   d004|1995-12-03|9999-01-01|\n",
      "| 10004|   d004|1986-12-01|9999-01-01|\n",
      "| 10005|   d003|1989-09-12|9999-01-01|\n",
      "| 10006|   d005|1990-08-05|9999-01-01|\n",
      "| 10007|   d008|1989-02-10|9999-01-01|\n",
      "| 10008|   d005|1998-03-11|2000-07-31|\n",
      "| 10009|   d006|1985-02-18|9999-01-01|\n",
      "| 10010|   d004|1996-11-24|2000-06-26|\n",
      "| 10010|   d006|2000-06-26|9999-01-01|\n",
      "| 10011|   d009|1990-01-22|1996-11-09|\n",
      "| 10012|   d005|1992-12-18|9999-01-01|\n",
      "| 10013|   d003|1985-10-20|9999-01-01|\n",
      "| 10014|   d005|1993-12-29|9999-01-01|\n",
      "| 10015|   d008|1992-09-19|1993-08-22|\n",
      "| 10016|   d007|1998-02-11|9999-01-01|\n",
      "| 10017|   d001|1993-08-03|9999-01-01|\n",
      "| 10018|   d004|1992-07-29|9999-01-01|\n",
      "| 10018|   d005|1987-04-03|1992-07-29|\n",
      "+------+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "+------+-------+----------+----------+\n",
      "|emp_no|dept_no| from_date|   to_date|\n",
      "+------+-------+----------+----------+\n",
      "|110022|   d001|1985-01-01|1991-10-01|\n",
      "|110039|   d001|1991-10-01|9999-01-01|\n",
      "|110085|   d002|1985-01-01|1989-12-17|\n",
      "|110114|   d002|1989-12-17|9999-01-01|\n",
      "|110183|   d003|1985-01-01|1992-03-21|\n",
      "|110228|   d003|1992-03-21|9999-01-01|\n",
      "|110303|   d004|1985-01-01|1988-09-09|\n",
      "|110344|   d004|1988-09-09|1992-08-02|\n",
      "|110386|   d004|1992-08-02|1996-08-30|\n",
      "|110420|   d004|1996-08-30|9999-01-01|\n",
      "|110511|   d005|1985-01-01|1992-04-25|\n",
      "|110567|   d005|1992-04-25|9999-01-01|\n",
      "|110725|   d006|1985-01-01|1989-05-06|\n",
      "|110765|   d006|1989-05-06|1991-09-12|\n",
      "|110800|   d006|1991-09-12|1994-06-28|\n",
      "|110854|   d006|1994-06-28|9999-01-01|\n",
      "|111035|   d007|1985-01-01|1991-03-07|\n",
      "|111133|   d007|1991-03-07|9999-01-01|\n",
      "|111400|   d008|1985-01-01|1991-04-08|\n",
      "|111534|   d008|1991-04-08|9999-01-01|\n",
      "+------+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|    Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|     Simmel|     F|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|    Bamford|     M|1986-08-28|\n",
      "| 10004|1954-05-01| Chirstian|    Koblick|     M|1986-12-01|\n",
      "| 10005|1955-01-21|   Kyoichi|   Maliniak|     M|1989-09-12|\n",
      "| 10006|1953-04-20|    Anneke|    Preusig|     F|1989-06-02|\n",
      "| 10007|1957-05-23|   Tzvetan|  Zielinski|     F|1989-02-10|\n",
      "| 10008|1958-02-19|    Saniya|   Kalloufi|     M|1994-09-15|\n",
      "| 10009|1952-04-19|    Sumant|       Peac|     F|1985-02-18|\n",
      "| 10010|1963-06-01| Duangkaew|   Piveteau|     F|1989-08-24|\n",
      "| 10011|1953-11-07|      Mary|      Sluis|     F|1990-01-22|\n",
      "| 10012|1960-10-04|  Patricio|  Bridgland|     M|1992-12-18|\n",
      "| 10013|1963-06-07| Eberhardt|     Terkki|     M|1985-10-20|\n",
      "| 10014|1956-02-12|     Berni|      Genin|     M|1987-03-11|\n",
      "| 10015|1959-08-19|  Guoxiang|  Nooteboom|     M|1987-07-02|\n",
      "| 10016|1961-05-02|  Kazuhito|Cappelletti|     M|1995-01-27|\n",
      "| 10017|1958-07-06| Cristinel|  Bouloucos|     F|1993-08-03|\n",
      "| 10018|1954-06-19|  Kazuhide|       Peha|     F|1987-04-03|\n",
      "| 10019|1953-01-23|   Lillian|    Haddadi|     M|1999-04-30|\n",
      "| 10020|1952-12-24|    Mayuko|    Warwick|     M|1991-01-26|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "+------+------+----------+----------+\n",
      "|emp_no|salary| from_date|   to_date|\n",
      "+------+------+----------+----------+\n",
      "| 10001| 60117|1986-06-26|1987-06-26|\n",
      "| 10001| 62102|1987-06-26|1988-06-25|\n",
      "| 10001| 66074|1988-06-25|1989-06-25|\n",
      "| 10001| 66596|1989-06-25|1990-06-25|\n",
      "| 10001| 66961|1990-06-25|1991-06-25|\n",
      "| 10001| 71046|1991-06-25|1992-06-24|\n",
      "| 10001| 74333|1992-06-24|1993-06-24|\n",
      "| 10001| 75286|1993-06-24|1994-06-24|\n",
      "| 10001| 75994|1994-06-24|1995-06-24|\n",
      "| 10001| 76884|1995-06-24|1996-06-23|\n",
      "| 10001| 80013|1996-06-23|1997-06-23|\n",
      "| 10001| 81025|1997-06-23|1998-06-23|\n",
      "| 10001| 81097|1998-06-23|1999-06-23|\n",
      "| 10001| 84917|1999-06-23|2000-06-22|\n",
      "| 10001| 85112|2000-06-22|2001-06-22|\n",
      "| 10001| 85097|2001-06-22|2002-06-22|\n",
      "| 10001| 88958|2002-06-22|9999-01-01|\n",
      "| 10002| 65828|1996-08-03|1997-08-03|\n",
      "| 10002| 65909|1997-08-03|1998-08-03|\n",
      "| 10002| 67534|1998-08-03|1999-08-03|\n",
      "+------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departments_df = spark.read.csv(\"D://Dataengineer//archive//departments.csv\" , header = True , inferSchema = True) \n",
    "dept_emp_df = spark.read.csv(\"D://Dataengineer//archive//dept_emp.csv\" , header = True , inferSchema = True)\n",
    "dept_manager_df = spark.read.csv(\"D://Dataengineer//archive//dept_manager.csv\" , header = True , inferSchema = True)\n",
    "employees_df = spark.read.csv(\"D://Dataengineer//archive//employees.csv\" , header = True , inferSchema = True)\n",
    "salaries_df = spark.read.csv(\"D://Dataengineer//archive//salaries.csv\" , header = True , inferSchema = True)\n",
    "\n",
    "departments_df.show()\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "dept_emp_df.show()\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "dept_manager_df.show()\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "employees_df.show()\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "salaries_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a77f9cf-5b82-40ae-9024-c081fc90fc7e",
   "metadata": {},
   "source": [
    "### Concanting first_name and last_name to full _name column and Dropping Columns First_name and last_name  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43b5ed9-b339-464d-af7b-b48593947c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+----------+--------------------+\n",
      "|emp_no|birth_date|gender| hire_date|           full_name|\n",
      "+------+----------+------+----------+--------------------+\n",
      "| 10001|1953-09-02|     M|1986-06-26|      Georgi Facello|\n",
      "| 10002|1964-06-02|     F|1985-11-21|      Bezalel Simmel|\n",
      "| 10003|1959-12-03|     M|1986-08-28|       Parto Bamford|\n",
      "| 10004|1954-05-01|     M|1986-12-01|   Chirstian Koblick|\n",
      "| 10005|1955-01-21|     M|1989-09-12|    Kyoichi Maliniak|\n",
      "| 10006|1953-04-20|     F|1989-06-02|      Anneke Preusig|\n",
      "| 10007|1957-05-23|     F|1989-02-10|   Tzvetan Zielinski|\n",
      "| 10008|1958-02-19|     M|1994-09-15|     Saniya Kalloufi|\n",
      "| 10009|1952-04-19|     F|1985-02-18|         Sumant Peac|\n",
      "| 10010|1963-06-01|     F|1989-08-24|  Duangkaew Piveteau|\n",
      "| 10011|1953-11-07|     F|1990-01-22|          Mary Sluis|\n",
      "| 10012|1960-10-04|     M|1992-12-18|  Patricio Bridgland|\n",
      "| 10013|1963-06-07|     M|1985-10-20|    Eberhardt Terkki|\n",
      "| 10014|1956-02-12|     M|1987-03-11|         Berni Genin|\n",
      "| 10015|1959-08-19|     M|1987-07-02|  Guoxiang Nooteboom|\n",
      "| 10016|1961-05-02|     M|1995-01-27|Kazuhito Cappelletti|\n",
      "| 10017|1958-07-06|     F|1993-08-03| Cristinel Bouloucos|\n",
      "| 10018|1954-06-19|     F|1987-04-03|       Kazuhide Peha|\n",
      "| 10019|1953-01-23|     M|1999-04-30|     Lillian Haddadi|\n",
      "| 10020|1952-12-24|     M|1991-01-26|      Mayuko Warwick|\n",
      "+------+----------+------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit , concat\n",
    "employees_df = employees_df.withColumn(\"full_name\" , concat(employees_df.first_name , lit(\" \") , employees_df.last_name))\n",
    "drop_columns = [\"first_name\" , \"last_name\"]\n",
    "employees_df = employees_df.drop(*drop_columns)\n",
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d01f2e5-2a51-4c23-a8c2-a330d7ad8be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departments_df.printSchema()\n",
    "employees_df.printSchema()\n",
    "dept_emp_df.printSchema()\n",
    "dept_manager_df.printSchema()\n",
    "salaries_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ae9bf2c-16f3-4744-bcaf-1456b1c936f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "duplicate_rows_count = dept_emp_df.count() - dept_emp_df.distinct().count()\n",
    "print(duplicate_rows_count)\n",
    "\n",
    "print(dept_manager_df.count() - dept_manager_df.distinct().count())\n",
    "print(employees_df.count() - employees_df.distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7b8b4e-39b8-4c40-bc52-da4ce95e6c3d",
   "metadata": {},
   "source": [
    "## Finding Duplicates in Salary Table\n",
    "##### Are there multiple salary records for the same employee and time period in the salaries table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11943ac8-33ac-4fd6-a47d-95662997b75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2844047\n",
      "2251965\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col , max , min , when , lag , lead \n",
    "from pyspark.sql import Window\n",
    "window = Window.partitionBy(\"emp_no\").orderBy(\"from_date\")\n",
    "salaries_df = salaries_df.withColumn(\"prev_to_date\" , lag(\"to_date\" , 1).over(window)) \\\n",
    "            .withColumn(\"next_from_date\" , lead(\"from_date\" , 1).over(window)).withColumn(\"prev_salary\", lag(\"salary\", 1).over(window))\n",
    "\n",
    "duplicates = salaries_df.filter(\n",
    "    (col('from_date') <= col(\"next_from_date\")) &\n",
    "    (col(\"prev_to_date\") <= col(\"to_date\")) &\n",
    "    (col(\"salary\") != col(\"prev_salary\"))\n",
    ")\n",
    "print(salaries_df.count())\n",
    "print(duplicates.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b398f52-fb02-4648-8a49-b8c3b5b34b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Function to check Nulls \n",
    "def get_null_count(cl , df):\n",
    "    from pyspark.sql.functions import col\n",
    "    null_counts = df.filter(col(cl).isNull()).count()\n",
    "    return null_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6778c-8327-433c-85ff-9995a4d77229",
   "metadata": {},
   "source": [
    "### Checking nulls in Salary DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9989ae5b-8857-4b70-8537-747950a4a6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(get_null_count(cl = \"emp_no\" , df= salaries_df))\n",
    "print(get_null_count(cl = \"from_date\" , df= salaries_df))\n",
    "print(get_null_count(cl = \"to_date\" , df= salaries_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078fce73-5dba-4858-aae5-d0e6160052ec",
   "metadata": {},
   "source": [
    "### Checking nulls in dept_emp , dept_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8cc52d0-f7ff-40b6-a536-16ef158ff7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(get_null_count(cl = \"dept_no\" , df= dept_emp_df))\n",
    "print(get_null_count(cl = \"dept_no\" , df= dept_manager_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bfccd4-ceba-4217-9fe9-a78c71b6c723",
   "metadata": {},
   "source": [
    "## Transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e542a-4cf7-4211-be25-c5be4427f00a",
   "metadata": {},
   "source": [
    "#### Calculate the Employee tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82f9a68b-4f12-4c62-84d1-b536e864b4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+----------+--------------------+------+\n",
      "|emp_no|birth_date|gender| hire_date|           full_name|tenure|\n",
      "+------+----------+------+----------+--------------------+------+\n",
      "| 10001|1953-09-02|     M|1986-06-26|      Georgi Facello| 38.19|\n",
      "| 10002|1964-06-02|     F|1985-11-21|      Bezalel Simmel| 38.78|\n",
      "| 10003|1959-12-03|     M|1986-08-28|       Parto Bamford| 38.02|\n",
      "| 10004|1954-05-01|     M|1986-12-01|   Chirstian Koblick| 37.76|\n",
      "| 10005|1955-01-21|     M|1989-09-12|    Kyoichi Maliniak| 34.98|\n",
      "| 10006|1953-04-20|     F|1989-06-02|      Anneke Preusig| 35.25|\n",
      "| 10007|1957-05-23|     F|1989-02-10|   Tzvetan Zielinski| 35.56|\n",
      "| 10008|1958-02-19|     M|1994-09-15|     Saniya Kalloufi| 29.97|\n",
      "| 10009|1952-04-19|     F|1985-02-18|         Sumant Peac| 39.54|\n",
      "| 10010|1963-06-01|     F|1989-08-24|  Duangkaew Piveteau| 35.03|\n",
      "| 10011|1953-11-07|     F|1990-01-22|          Mary Sluis| 34.62|\n",
      "| 10012|1960-10-04|     M|1992-12-18|  Patricio Bridgland| 31.71|\n",
      "| 10013|1963-06-07|     M|1985-10-20|    Eberhardt Terkki| 38.87|\n",
      "| 10014|1956-02-12|     M|1987-03-11|         Berni Genin| 37.48|\n",
      "| 10015|1959-08-19|     M|1987-07-02|  Guoxiang Nooteboom| 37.17|\n",
      "| 10016|1961-05-02|     M|1995-01-27|Kazuhito Cappelletti|  29.6|\n",
      "| 10017|1958-07-06|     F|1993-08-03| Cristinel Bouloucos| 31.08|\n",
      "| 10018|1954-06-19|     F|1987-04-03|       Kazuhide Peha| 37.42|\n",
      "| 10019|1953-01-23|     M|1999-04-30|     Lillian Haddadi| 25.34|\n",
      "| 10020|1952-12-24|     M|1991-01-26|      Mayuko Warwick|  33.6|\n",
      "+------+----------+------+----------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date , months_between , datediff , round\n",
    "employees_df = employees_df.withColumn(\"tenure\" , round(months_between(current_date() , col('hire_date'))/12 , 2))\n",
    "employees_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a785f3-6d9d-49bc-a1a9-c6ad80f82bcb",
   "metadata": {},
   "source": [
    "### Calculate longest server employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e1687e56-f6d8-4dcf-bc06-a4353eed0840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(full_name='Margareta Markovitch', tenure=39.67)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "employees_df.sort(employees_df.tenure.desc()).select('full_name' , 'tenure').head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "166f22c1-f10e-4dd0-8282-2972bf48af8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function max in module pyspark.sql.functions:\n",
      "\n",
      "max(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "    Aggregate function: returns the maximum value of the expression in a group.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    col : :class:`~pyspark.sql.Column` or str\n",
      "        target column to compute on.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`~pyspark.sql.Column`\n",
      "        column for computed results.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.range(10)\n",
      "    >>> df.select(max(col(\"id\"))).show()\n",
      "    +-------+\n",
      "    |max(id)|\n",
      "    +-------+\n",
      "    |      9|\n",
      "    +-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd41b40-21f2-4bb7-a642-19e4d3b203ca",
   "metadata": {},
   "source": [
    "## Calculate increase Salary Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c9672f5-12ea-4cc3-95d4-331bc4c12497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+------------+--------------+-----------+---------------+\n",
      "|emp_no|salary| from_date|   to_date|prev_to_date|next_from_date|prev_salary|salary_increase|\n",
      "+------+------+----------+----------+------------+--------------+-----------+---------------+\n",
      "| 10010| 72488|1996-11-24|1997-11-24|        NULL|    1997-11-24|       NULL|            0.0|\n",
      "| 10010| 74347|1997-11-24|1998-11-24|  1997-11-24|    1998-11-24|      72488|           2.56|\n",
      "| 10010| 75405|1998-11-24|1999-11-24|  1998-11-24|    1999-11-24|      74347|           1.42|\n",
      "| 10010| 78194|1999-11-24|2000-11-23|  1999-11-24|    2000-11-23|      75405|            3.7|\n",
      "| 10010| 79580|2000-11-23|2001-11-23|  2000-11-23|    2001-11-23|      78194|           1.77|\n",
      "| 10010| 80324|2001-11-23|9999-01-01|  2001-11-23|          NULL|      79580|           0.93|\n",
      "| 10011| 42365|1990-01-22|1991-01-22|        NULL|    1991-01-22|       NULL|            0.0|\n",
      "| 10011| 44200|1991-01-22|1992-01-22|  1991-01-22|    1992-01-22|      42365|           4.33|\n",
      "| 10011| 48214|1992-01-22|1993-01-21|  1992-01-22|    1993-01-21|      44200|           9.08|\n",
      "| 10011| 50927|1993-01-21|1994-01-21|  1993-01-21|    1994-01-21|      48214|           5.63|\n",
      "| 10011| 51470|1994-01-21|1995-01-21|  1994-01-21|    1995-01-21|      50927|           1.07|\n",
      "| 10011| 54545|1995-01-21|1996-01-21|  1995-01-21|    1996-01-21|      51470|           5.97|\n",
      "| 10011| 56753|1996-01-21|1996-11-09|  1996-01-21|          NULL|      54545|           4.05|\n",
      "| 10013| 40000|1985-10-20|1986-10-20|        NULL|    1986-10-20|       NULL|            0.0|\n",
      "| 10013| 40623|1986-10-20|1987-10-20|  1986-10-20|    1987-10-20|      40000|           1.56|\n",
      "| 10013| 40561|1987-10-20|1988-10-19|  1987-10-20|    1988-10-19|      40623|           0.15|\n",
      "| 10013| 40306|1988-10-19|1989-10-19|  1988-10-19|    1989-10-19|      40561|           0.63|\n",
      "| 10013| 43569|1989-10-19|1990-10-19|  1989-10-19|    1990-10-19|      40306|            8.1|\n",
      "| 10013| 46305|1990-10-19|1991-10-19|  1990-10-19|    1991-10-19|      43569|           6.28|\n",
      "| 10013| 47118|1991-10-19|1992-10-18|  1991-10-19|    1992-10-18|      46305|           1.76|\n",
      "+------+------+----------+----------+------------+--------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import abs , when ,lit\n",
    "win = Window.partitionBy(\"emp_no\").orderBy(\"from_date\")\n",
    "salaries_df = salaries_df.withColumn(\"prev_salary\" ,lag(\"salary\" , 1).over(win))\n",
    "salaries_df = salaries_df.withColumn(\"salary_increase\" , \\\n",
    "                                     when(col(\"prev_salary\").isNotNull() ,\n",
    "                                          round(abs((col(\"prev_salary\")- col(\"salary\")) / col(\"prev_salary\")) * 100 , 2)).otherwise(lit(0)))\n",
    "salaries_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe993d-31d8-48df-b160-163725cda425",
   "metadata": {},
   "source": [
    "### Calculate Average salary deparment wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99b83644-e515-4b05-bdd0-4fedc4fb9003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|         dept_name|        avg_salary|\n",
      "+------------------+------------------+\n",
      "|             Sales| 80667.60575533769|\n",
      "|Quality Management| 57251.27191341599|\n",
      "|           Finance| 70489.36489699609|\n",
      "|        Production|59605.482461651445|\n",
      "|          Research|  59665.1817012686|\n",
      "|  Customer Service| 58770.36647976248|\n",
      "|         Marketing| 71913.20000419153|\n",
      "|       Development| 59478.90116243182|\n",
      "|   Human Resources| 55574.87936969553|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "Avg_salary = salaries_df.join(dept_emp_df.select(\"emp_no\" , \"dept_no\") , \"emp_no\").join(departments_df.select(\"dept_no\" , \"dept_name\") , \"dept_no\")\n",
    "Avg_salary.groupBy('dept_name').agg(avg('salary').alias('avg_salary')).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174218b5-ac59-43c9-a177-3276f1570156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dea890b-19b1-498c-9fc7-6e90ef47110d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde128ee-c270-4e91-b75b-fa69138a527b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5999721f-fc0a-4120-99ac-9ea0fe3c4165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function months_between in module pyspark.sql.functions:\n",
      "\n",
      "months_between(date1: 'ColumnOrName', date2: 'ColumnOrName', roundOff: bool = True) -> pyspark.sql.column.Column\n",
      "    Returns number of months between dates date1 and date2.\n",
      "    If date1 is later than date2, then the result is positive.\n",
      "    A whole number is returned if both inputs have the same day of month or both are the last day\n",
      "    of their respective months. Otherwise, the difference is calculated assuming 31 days per month.\n",
      "    The result is rounded off to 8 digits unless `roundOff` is set to `False`.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    date1 : :class:`~pyspark.sql.Column` or str\n",
      "        first date column.\n",
      "    date2 : :class:`~pyspark.sql.Column` or str\n",
      "        second date column.\n",
      "    roundOff : bool, optional\n",
      "        whether to round (to 8 digits) the final value or not (default: True).\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`~pyspark.sql.Column`\n",
      "        number of months between two dates.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n",
      "    >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()\n",
      "    [Row(months=3.94959677)]\n",
      "    >>> df.select(months_between(df.date1, df.date2, False).alias('months')).collect()\n",
      "    [Row(months=3.9495967741935485)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(months_between)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f8ecb-d720-4561-a3a8-09267002ec5a",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858075bb-0f7e-4442-b4a0-e24c0371c675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75513c10-e591-4dea-befe-a102ffcb0ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47581b0-5dfd-4568-b100-687bdf29f984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
